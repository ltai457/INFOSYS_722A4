{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb76b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/22 11:45:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tree_methods_adv').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190fe461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"./Dataset/MegerdData.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887800af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Patient ID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- Systolic_BP: integer (nullable = true)\n",
      " |-- Diastolic_BP: integer (nullable = true)\n",
      " |-- Heart Rate: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Family History: integer (nullable = true)\n",
      " |-- Smoking: integer (nullable = true)\n",
      " |-- Obesity: integer (nullable = true)\n",
      " |-- Alcohol Consumption: integer (nullable = true)\n",
      " |-- Exercise Hours Per Week: double (nullable = true)\n",
      " |-- Diet: string (nullable = true)\n",
      " |-- Previous Heart Problems: integer (nullable = true)\n",
      " |-- Medication Use: integer (nullable = true)\n",
      " |-- Stress Level: string (nullable = true)\n",
      " |-- Sedentary Hours Per Day: double (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Triglycerides: integer (nullable = true)\n",
      " |-- Physical Activity Days Per Week: integer (nullable = true)\n",
      " |-- Sleep Hours Per Day: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Hemisphere: string (nullable = true)\n",
      " |-- Heart Attack Risk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get an idea of what the data looks like. \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ea5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 11:45:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Patient ID='BMW7812', Age=67, Sex='Male', Cholesterol=208, Systolic_BP=158, Diastolic_BP=88, Heart Rate=72, Diabetes=0, Family History=0, Smoking=1, Obesity=0, Alcohol Consumption=0, Exercise Hours Per Week=4.168188835, Diet='Average', Previous Heart Problems=0, Medication Use=0, Stress Level='99', Sedentary Hours Per Day=6.615001453, Income=261404, BMI=31.25123273, Triglycerides=286, Physical Activity Days Per Week=0, Sleep Hours Per Day=None, Country='Argentina', Continent='South America', Hemisphere='Southern Hemisphere', Heart Attack Risk=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc61a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few things we need to do before Spark can accept the data!\n",
    "# It needs to be in the form of two columns: \"label\" and \"features\".\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eceb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Patient ID',\n",
       " 'Age',\n",
       " 'Sex',\n",
       " 'Cholesterol',\n",
       " 'Systolic_BP',\n",
       " 'Diastolic_BP',\n",
       " 'Heart Rate',\n",
       " 'Diabetes',\n",
       " 'Family History',\n",
       " 'Smoking',\n",
       " 'Obesity',\n",
       " 'Alcohol Consumption',\n",
       " 'Exercise Hours Per Week',\n",
       " 'Diet',\n",
       " 'Previous Heart Problems',\n",
       " 'Medication Use',\n",
       " 'Stress Level',\n",
       " 'Sedentary Hours Per Day',\n",
       " 'Income',\n",
       " 'BMI',\n",
       " 'Triglycerides',\n",
       " 'Physical Activity Days Per Week',\n",
       " 'Sleep Hours Per Day',\n",
       " 'Country',\n",
       " 'Continent',\n",
       " 'Hemisphere',\n",
       " 'Heart Attack Risk']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualise the columns to help with assembly. \n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bb57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into one vector named features.\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=[\n",
    " 'Age',\n",
    " \n",
    " 'Cholesterol',\n",
    " 'Systolic_BP',\n",
    " 'Diastolic_BP',\n",
    " 'Heart Rate',\n",
    " 'Diabetes',\n",
    " 'Family History',\n",
    " 'Smoking',\n",
    " 'Obesity',\n",
    " 'Alcohol Consumption',\n",
    " 'Exercise Hours Per Week',\n",
    " \n",
    " 'Previous Heart Problems',\n",
    " 'Medication Use',\n",
    " \n",
    " 'Sedentary Hours Per Day',\n",
    " 'Income',\n",
    " 'BMI',\n",
    " 'Triglycerides',\n",
    " 'Physical Activity Days Per Week',\n",
    " 'Sleep Hours Per Day',\n",
    " \n",
    " ],\n",
    "              outputCol=\"features\",handleInvalid=\"keep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c8d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform the data. \n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de8907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the string indexer (similar to the logistic regression exercises).\n",
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c06d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"Heart Attack Risk\", outputCol=\"Heart Attack Risk Index\")\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de7d1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Heart Attack Risk Index: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, Heart Attack Risk Index: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n",
    "final_data = output_fixed.select(\"features\",'Heart Attack Risk Index')\n",
    "final_data.printSchema()\n",
    "display(final_data)\n",
    "# Split the training and testing set.\n",
    "train_data,test_data = final_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895a5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d269815",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='Heart Attack Risk Index',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='Heart Attack Risk Index',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='Heart Attack Risk Index',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f494118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the models (it's three models, so it might take some time).\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9084ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449b02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'Heart Attack Risk Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e1aba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5016571618037134\n",
      "RFC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5032599469496013\n",
      "GBT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 11:54:03 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/22 11:54:04 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49576259946949597\n"
     ]
    }
   ],
   "source": [
    "# This is the area under the curve. This indicates that the data is highly seperable.\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n",
    "# We can't repeat these exact steps for GBT. If you print the schema of all three, you may be able to notice why.\n",
    "# Instead, let's redefine the object:\n",
    "my_binary_gbt_eval = BinaryClassificationEvaluator(labelCol='Heart Attack Risk Index', rawPredictionCol='prediction')\n",
    "print(\"GBT\")\n",
    "print(my_binary_gbt_eval.evaluate(gbt_predictions))\n",
    "\n",
    "# Interesting, GBT didn't perform as well as RFC or DTC. But that's because we left the model's settings as default. \n",
    "# In most cases, we should adjust these parameters. More trees may increase accuracy, but decrease precision and recall. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2280fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd47596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"Heart Attack Risk Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "032c8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1b5b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 62.43%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 64.09%\n",
      "----------------------------------------\n",
      "An ensemble using GBT has an accuracy of: 61.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 12:02:59 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 121327 ms exceeds timeout 120000 ms\n",
      "24/05/22 12:08:02 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644db51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fda51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53805e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cac88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
